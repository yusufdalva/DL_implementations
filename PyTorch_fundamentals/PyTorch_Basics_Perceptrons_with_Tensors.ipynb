{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Basics: Perceptrons with Tensors.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjSdDvr5rCuAhynMzoWnpZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusufdalva/ML_implementations/blob/torch_prod/PyTorch_fundamentals/PyTorch_Basics_Perceptrons_with_Tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGJpKmsDNZjw"
      },
      "source": [
        "# PyTorch Basics\n",
        "This notebook shows applying basic calculations on PyTorch (numpy like operators). The implementation is originated from the lectures in https://www.udacity.com/course/deep-learning-pytorch--ud188. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx7tHZvhNBRM"
      },
      "source": [
        "# Importing PyTorch\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCt-gIX1N9Dm"
      },
      "source": [
        "## The Sigmoid Activation Function\n",
        "As the first function, the sigmoid activation function is implemented in PyTorch. <br>The formulation is as follows: $\\large{\\sigma(x) = \\frac{1}{1 + e^{-z}}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf_6es-tN5Sc"
      },
      "source": [
        "def sigmoid_activation(z):\n",
        "  \"\"\" Implementation of sigmoid activation function.\n",
        "  Inputs:\n",
        "  - z: torch.Tensor \"\"\"\n",
        "  return 1 / (1 + torch.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9jLl3NKRa7I"
      },
      "source": [
        "Now to test the sigmoid function and a perceptron in general, the linear discriminant function will be implemented, which basically performs: <br><br> $\\large{(\\sum_{i=1}^{n}{W_{i}.X_{i}) + b}}$, where n represents the number of features here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-u5dydnPeSN"
      },
      "source": [
        "# Generation test input\n",
        "torch.manual_seed(7) # Just like np.radom.seed in numpy\n",
        "\n",
        "NO_OF_FEATURES = 5\n",
        "NO_OF_SAMPLES = 1\n",
        "\n",
        "X = torch.randn((NO_OF_SAMPLES,NO_OF_FEATURES)) # Single data sample for the beginning\n",
        "W = torch.randn_like(X) # Random initial weights - Weights and input features have the same shape, weights for a single neuron\n",
        "b = torch.randn((NO_OF_SAMPLES,1)) # Random initial bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyY3VpE4Emg9"
      },
      "source": [
        "For the implementation of the perceptron, first the linear discriminant function is calculated. Then the sigmoid activation function is applied to this output. A figure illustrating this is shown below: <img src=\"https://i.stack.imgur.com/7mTvt.jpg\"/><br> Here the function *f* is sigmoid activation function. Credit: https://i.stack.imgur.com/7mTvt.jpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DorypAUISiRG"
      },
      "source": [
        "def sigmoid_perceptron(X, W, b):\n",
        "  sum = torch.matmul(X, W) + b\n",
        "  return sigmoid_activation(sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO13hlhZTUKQ",
        "outputId": "c5751571-56af-47a3-af14-fcc31b49ec91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Printing the inputs and computing the output\n",
        "print('Features: ' + str(X))\n",
        "print('Weights: ' + str(W))\n",
        "print('Bias: ' + str(b))\n",
        "y_hat = sigmoid_perceptron(X, W.T, b)\n",
        "print('Output: ' + str(y_hat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
            "Weights: tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n",
            "Bias: tensor([[0.3177]])\n",
            "Output: tensor([[0.1595]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkQvpzNUaHTb"
      },
      "source": [
        "To change the shape of a tensor, the *view* function can be used. This function copies the original tensor and return the copy with the input shape: **torch.Tensor.view(input_shape)** - input is not a tuple, just the dimensions in order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmWyiHxzZMK9",
        "outputId": "33c47f90-2298-4f66-cdaf-96d891824d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_2 = sigmoid_activation(torch.sum(torch.matmul(X, W.view(NO_OF_FEATURES, NO_OF_SAMPLES))) + b)\n",
        "print(y_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1595]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ErwVT5a6ni"
      },
      "source": [
        "## Implementing a Hidden Layer\n",
        "For illustration purposes, a hidden layer is implemented with tensor operations. The implemented network (Having two layers - one hidden, one output layer) uses sigmoid activation via **sigmoid_perceptron** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDA46kx6a6MJ"
      },
      "source": [
        "# Generating initial weights for the hidden layer\n",
        "torch.manual_seed(7)\n",
        "\n",
        "NO_OF_FEATURES = 3\n",
        "NO_OF_SAMPLES = 1\n",
        "\n",
        "# Generating random input\n",
        "X = torch.randn((NO_OF_SAMPLES, NO_OF_FEATURES))\n",
        "\n",
        "# Specifiying the hidden layer properties\n",
        "input_units = X.shape[1]\n",
        "hidden_units = 2\n",
        "output_units = 1\n",
        "\n",
        "# Initialize the Weights and Biases for the network - Layer 1: 2 units(neurons), Layer 2(output): 1 unit(neuron)\n",
        "W1 = torch.randn(input_units, hidden_units) # Weights for the first layer\n",
        "W2 = torch.randn(hidden_units, output_units) # Weights for the second layer\n",
        "\n",
        "# One bias term for each unit (neuron)\n",
        "b1 = torch.randn((1, hidden_units))\n",
        "b2 = torch.randn((1, output_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY1U0DfSeo4x",
        "outputId": "c8847cda-2482-4b57-9687-0a34bc66f69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Computing the output\n",
        "out_1 = sigmoid_perceptron(X, W1, b1)\n",
        "print('Hidden layer output: ' + str(out_1))\n",
        "out_final = sigmoid_perceptron(out_1, W2, b2)\n",
        "print('Output of the model: ' + str(out_final))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden layer output: tensor([[0.6813, 0.4355]])\n",
            "Output of the model: tensor([[0.3171]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv3OxbSpA8Lk"
      },
      "source": [
        "## Tensor To Numpy Conversion\n",
        "In this part the conversion between tensors and numpy arrays are shown. Initially a 5x4 matrix is created from random values from a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBURqcGA7nx",
        "outputId": "766e8fd8-365e-4437-bbb8-ededd325bb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sample_arr = np.random.rand(5,4)\n",
        "print('Type of the object: ' + str(type(sample_arr)))\n",
        "print('Matrix contents:\\n' + str(sample_arr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the object: <class 'numpy.ndarray'>\n",
            "Matrix contents:\n",
            "[[0.16920867 0.3604626  0.12064992 0.35487684]\n",
            " [0.04610831 0.02638534 0.16433284 0.02154747]\n",
            " [0.51000063 0.2774759  0.62431993 0.02006936]\n",
            " [0.2186867  0.63957207 0.49931445 0.22757941]\n",
            " [0.40159413 0.20058523 0.50815272 0.6627985 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiA22Iz4DmNn"
      },
      "source": [
        "- Conversion from *Numpy array* to *torch.Tensor*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_qhlcqlBxJV",
        "outputId": "b6e50884-1f98-4042-b701-a0a2680d5e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "sample_tensor = torch.from_numpy(sample_arr)\n",
        "print('Type of the object: ' + str(type(sample_tensor)))\n",
        "print('Matrix contents:\\n' + str(sample_tensor))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the object: <class 'torch.Tensor'>\n",
            "Matrix contents:\n",
            "tensor([[0.1692, 0.3605, 0.1206, 0.3549],\n",
            "        [0.0461, 0.0264, 0.1643, 0.0215],\n",
            "        [0.5100, 0.2775, 0.6243, 0.0201],\n",
            "        [0.2187, 0.6396, 0.4993, 0.2276],\n",
            "        [0.4016, 0.2006, 0.5082, 0.6628]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B4ordbDDyVR"
      },
      "source": [
        "- Conversion from *torch.Tensor* to *Numpy array*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CX5rlUmCq1-",
        "outputId": "33925934-ff82-40ed-8f97-283662818fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print('Type of the object: ' + str(type(sample_tensor.numpy())))\n",
        "print('Matrix contents:\\n' + str(sample_tensor.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the object: <class 'numpy.ndarray'>\n",
            "Matrix contents:\n",
            "[[0.16920867 0.3604626  0.12064992 0.35487684]\n",
            " [0.04610831 0.02638534 0.16433284 0.02154747]\n",
            " [0.51000063 0.2774759  0.62431993 0.02006936]\n",
            " [0.2186867  0.63957207 0.49931445 0.22757941]\n",
            " [0.40159413 0.20058523 0.50815272 0.6627985 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNF1B4uRD6Rp"
      },
      "source": [
        "- Both of the data structures (Tensor and ndarray) point to the same data, manipulation done by one of them effects the other one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuG7J1KBC71d",
        "outputId": "9056ce0d-f037-42c4-8a41-246a281b63e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "sample_tensor.add_(5)\n",
        "print('Tensor contents:\\n' + str(sample_tensor))\n",
        "print('Numpy array contents:\\n' + str(sample_arr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor contents:\n",
            "tensor([[5.1692, 5.3605, 5.1206, 5.3549],\n",
            "        [5.0461, 5.0264, 5.1643, 5.0215],\n",
            "        [5.5100, 5.2775, 5.6243, 5.0201],\n",
            "        [5.2187, 5.6396, 5.4993, 5.2276],\n",
            "        [5.4016, 5.2006, 5.5082, 5.6628]], dtype=torch.float64)\n",
            "Numpy array contents:\n",
            "[[5.16920867 5.3604626  5.12064992 5.35487684]\n",
            " [5.04610831 5.02638534 5.16433284 5.02154747]\n",
            " [5.51000063 5.2774759  5.62431993 5.02006936]\n",
            " [5.2186867  5.63957207 5.49931445 5.22757941]\n",
            " [5.40159413 5.20058523 5.50815272 5.6627985 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}